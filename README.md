# Data-analytics
## Module 1 : The Basics of Data
- chapter 1 : The Data Analyst
- Chapter 2 : Understanding Data

## Module 2: Data Preparation and exploration
- Chapter 3: Databases and data Acquisition
- Chapter 4: Data Quality
- Chapter 5: Data Analysis and Statistics

## Module 3: Data Preparation and exploration
- Chapter 6 : Data Analytics tools
- Chapter 7 : Data Visualization with reports and Dashboards

## Module 4 : Data Governance
- Chapter 8: Defining Data Governance

# MODULE 1 : THE BASICS OF DATA
## Chapter 1: The data Aa

# Data Analytics
Domain of data analytics can be branched into two primary components:
* conceptual understanding of data (theoretical aspect)
* Hands-on manipulation of data (practical aspect)

* The aim is to extract actionable insights and valuable information from data
* What to understand under the Module:
  - Understand what the role of a data analyst is and the scope of data analytics
  - Compare and contrast different data types
  - Compare and contrast common data structures and file formats
  - Learner Assumptions

# What is Data Analytics
so virtually every organization collects large quantities of data about its customers,products,employees and service offerings
* Managers naturally seek to analyze that data and harness the information it contains to improve the efficiency ,effectiveness and profitability of their work>
* So what is the role of a data analyst ,is to **transform raw data into actionable insights that guide decision-making process within the organization**

## 1. Data Collection and Preparation:

* Sourcing Data from various channels,including databases ,spreadsheets and eternal sources
* Cleaning and organizing the data to ensure it is accurate consistent and ready for analysis

## 2. Data Analysis
* Employing statistical methods,machine learning techniques ,or other analytic tools to interpret data
* Identifying trends,patterns ,and correlations that might not be obvious

## 3. Data Visualization and Storytelling:
* having to create visual representations of the data,such as charts,graphs and dashboards to make complex information easily understandable
* Articulating findings in a compelling narrative to communicate the significance of data to stakeholders.

## 4. Decision Support
*Making recommendations based on data driven insights to help guide business decisions
* Providing more context around data, including potential implications and future trends.

## 5. Collaboration and Communication
* Working closely with other departments,such as **marketing** ,**finance and operations**,to understand their data needs and provide insights
* Adapting to new types of data and analytical methods as the organization's needs evolve.

A data analyst helps an organization make informed decisions that can improve operational efficiencies ,drive business strategies ,and create a competitive advantage.Goal is to contribute to the organization's success by turning data into valuable assets that informs and drives decision making

* Data analyst posses skills and knowledge required to perform this vital work
  - They often understand how organization can acquire,clean,and transform data to meet the organization's needs.
  - They are able to take the collected information and analyze it using techniques of statistics and machine learning
  - They may then create powerful visualizations that display this data to business leaders,managers and other stakeholders

 # 5 Key points of data analytics

 ### step 1
 process of analyzing raw data to produce useful insights to drive smart business decision,it is kind of business intelligence that can be used to find patterns,example how customers engage with a particualr product.
 ### Step 2 How data analytics is used
 use to make faster and better business decisons
 - can be used to reduce overall business costs
 - Develop new and innovative products and services

* can be used to predict future sales or purchasinbg behaviours
* To help and protect against fraud
* To analyze the effectiveness of marketing campaigns
* To boost customer acquisition and retention
* To increase Supply chain Efficiency

### Step 3 What does a data Analyst do on a day to day?
#### Roles and Responsibilities
* Manage the delivery of user satisfaction surveys and report on results using data visualization software
* Work with business line owners to develop requirements ,define success metrics,manage and execute analytical projects and evaluate results
* monitor practices,processes and systems to identify opportunities for improvement
* Translate important questions into concrete analytical tasks
* Gather new data to answer client questions ,collating and organizing data from multiple sources
* Design ,Build, Test and maintain backend code
* Establish data processes,define data quality criteria,and implement data quality processes
* Work as part of a team to evaluate and analyze key data that will be used to shape future business strategies

### Step 4: Typical process of a data analyst
##### 5 steps data analyst can take when approaching a new projects

* ``Step 1: Define questions you want to answer=take problem,make hypothesis``
  - what data will I need?
  - Where will it come from?
    
* ``Step 2: Collect the data``
  - gather data from primary sources or from internal data
    - from CRM Software ,Email ,marketing tools
    - can also gather data from secondary sources or external sources
      
* ``Step 3 : Clean the Data``
  - original data might include duplicates ,anamoalies ,missing data,this must be removed
    
* ``Step 4 : Analyze the Data``
  - common analysis include:
    - Regression analysis
    - Cluster analysis
    - Time series analysis

* ``Step 5 : intepret and share results``
  - in a form of a chart or graph,and being able to say what the data analysis shows you.

### Step 5: Skills needed to become a data Analyst
- Mathematical and statistical ability
- programming knowledge on python ,oracle and SQL
- Analytical mindset
- Excellenct communication skills
 
* Business analytics is all about answering questions and solving business challenges
---
---

### Three Major pillars that allow analytics progras to thrive
* Data
* Storage
* Computing power

## The analytics Process
- Analysts working with data move through a series of different steps as they seek to gain business value from their organization's data
![Screenshot 2024-04-09 105600](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/9a12f9f1-1010-4645-9dc0-eba382930289)

* Data acquisition
* Cleaning and manipulation
* Analysis
* Visualization
* Reporting and communication

#### Analytics  Process is iterative
Steps of the analytics process as aseries of sequential actions,it is more accurate to think of them as aset of interrelated actions that may be revisited frequently while working with a dataset

## Analytics Techniques
As analysts we use variety of techniques to draw conclusions from the data at their disposal.
- ``Descriptive Analytics``
- ``Predictive Analytics``
- ``Prescriptive analytics``

## Machine Learning,Artificial Intelligence,and Deep Learning
* Work of analytics is intellectually and computationally demanding.
* Machine Learning uses algorithms to discover knowledge in your datasets that you can then apply to help one make informed decisions about the future

**Cases Where Machine Learning Commonly adds value:**
- Segmenting customers and determining the marketing messages that will appeal to different customer groups.
- Discovering anomalies in system and application logs that may be indicative of a cybersecurity incident
- Forecasting product sales based on market and environmental conditions

* ``Artificial Intelligence(AI)`` includes any type of technique whee you are attempting to get a compuer system to imitate human behavior
* ``Machine Learning(ML)`` is a subset of AI techniques,ML techniques attempt to apply statistics to data problems in an effort to discover new knowledge,ML techniques are AI techniques designed to learn.
* ``Deep Learning`` is a further subdivision of machine learning that uses quite complex techniques,known as neural networks ,it is a highly specialized subfield of machine learning that is most commonly used for image and sound analysis.
![Screenshot 2024-04-09 111938](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/7b80094c-fdd9-4dcd-96d2-e3e9bd26f8cb)

## DATA GOVERNANCE
Data Governance programs ensure that the organization has high-quality data and is able to effectively control that data

## ANALYTICS TOOLS
These tools automate much of the heavy lifting of data analysis,improving the analyst's ability to
* **``acquire``**
* **``clean``**
* **``manipulate``**
* **``visualize and analyze data``**
* **``Invaluable assistance in reporting and communicating results``**
---
---


# Chapter 2: Understanding Data
## Exploring Data Types 

* Data element is an attribute about a person,place ,or thing containing data within a range of values.Data elements also describe characteristics of activities ,including orders ,transactions and events.,the column headings name the data element ,while each row is an example value for that format
* Data type limits the values a data element can have ,individual data types support structured,unstructured and semi-structured data

* **1 Tabular Data**
  - tabular data is data organized into a table,made up of columns and rows .a table represents information about a single topic
  - Each **``column``** represents a uniquely named field within a table,also called a variable,about a single characteristics,contents of each column contain values for the data element as defined by the column header
  - Each **``row``** represents a record of a single instance of the table's topic
* Tabular data as rectangular ,meaning it is easier to to draw a rectangle around.top of the rectangle is defined by columns,while rows define the left side of the rectangle,and the intersection of a row and colum contains a specific value.
* Microsoft Excel ,google sheets, and apple numbers are practical tools for presenting tabular data
* Relational Database management system(RDMS), commonly called a database extends the tabular model wherby instead of having all data in a single table,a database organisez related data across multiple tables
* Instead of having all data in a single table,a database organizes related data across multiple tables
* The connection between tables is known as a relationship.Oracle,Microsoft SQL server,MySQL, and PostgreSQL are some examples of database software
* Tabular data is the concept that underpins both spreadsheets and relational databases

## STRUCTURED DATA TYPES
* Structured data is tabular in nature and is organized into rows and columns,Structured data is what typically comes to mind when looking at a spreadsheet
* Structured data is what typically comes to mind when looking at a spreadsheet.With clearly defined columnn,headings,spreadsheets are easy to work with and easy to understand,so in a spreadsheet cells are where rows and colums intersect.

## Common Data Types that give structured data its structure 

## Character
character data type limits data entry to only valid characters .Characters can include the alphabet that you might see on your keyboard,as well as numbers.So depending on your needs multiple data types are available that can enforce character limits

* **ALPHANUMERIC** is the most widely used data type for strong character based data,alphanumeric is appropriate when a data element consists of both numbers and letters
* Alphanumeric data type is ideal for storing product stock-keeping units (SKUs),skus are common for retail clothing space
* one practical use of the text data type is to improve the overall data quality

![Screenshot 2024-04-09 153302](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/7f802c20-9ec6-48a4-8756-56e8aa94fe99)

## Character Sets
* We need to think about the character set we are using to input and store data when using a database.
* **Database use character sets to map,or encode,data and store it digitally**
* The ASCII encoding standard is based on the U.S English alphabet,it accommodates both the upper and lowercase English alphabet and numbers,mathematical operators and symbols
* one of the most commonis unicode Transormation Format-8(UTF-8), it allows non latin characters to be input by a user and stored in a file or database

## DATA TYPE
#### NUMBERS
* When numbers exclusively make up values for a data attribute,numeric becomes the data type of choice
* This data type appears to be simple and obvious based on its name.
* Databases accommodate two types of numeric data types:integar and numeric.

#### Whole Numbers
* The integar and all its subtypes are for storing whole numbers
* The bit data type is intended for storing the status of a flag.

#### Rational Numbers
* The numeric data type is for rational numbers that include a decimal point.
* As with integar family of data types,each database vendor has its implementation nuances
* must take several factors into account when dealing with rational numbers.in both SQL Server and MySQL,there is a data type called numeric that is functionally equivalent to the decimal data type.

#### Date and Time
* Gathered together under the broad category of date,day of year and time of day are data elements that appear with great frequency
![Screenshot 2024-04-09 155648](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/c11885dd-4420-4782-ba73-f6da01f04316)

## Strong and Weak Typing
* Data Types define values placed in columns,Strong typing is when technology rigidly enforces data types.A database column defined as numeric only accepts numerical values
* Weak Typing loosely enforces data types ,spreadsheets use weak typing to help make it easier for people to finish their work,
* Spreadsheets default to an "automatic" data type and accommodate practically any value
---
---

# Unstructured data TYPES
* UNSTRUCTURED DATA is any type of data that does not fit neatly into the tabular model
* Examples of unstructured data include:
  - Digital images
  - audio recordings
  - video recordings
  - open-ended survey responses
* Analyzing unstructured data creates a wealth of information and insight.
* Unstructured data type:
* **``Binary``**
  - most common data types for storing unstructure data.It supports any type of digital file you may have Microsoft Excel spreadsheets to digital photographs.
  - When considering which binary data type to use ,file size tends to be the limiting factor.
  - We need to select a data type that is as large as the largest file you plan on storing

* **``Audio``**
  - it can come from variety of sources
  - The impact of capturing,storing,and analyzing audio data has led to the development of avalanche detection systems,These systems listen for and detect the acoustic characteristics of an avalanche,with real time notification capabilities,these systems reduces time it takes for emergency services to respond and alert hikers to treacherous condition
  - Audio can be stored in its raw form which consumes most storage space.
  - alternatively it can be encoded with compression algorithm to reduce the amount of space that is required
  - Regardless of it it is in raw or compressed form,storing audio requires a data type designed to handle raw binary data
* **``Images``**
  - image data has applicability across several industries.
  - Resolution is the most significant factor that governs how much space is required to store an image.The greater the resolution the more detail an image contains,and the more storage space it needs
  - Similar to compressing audio data ,there are a variety of ways to encode and store images
  - Storing images in a database requires a data type designed to handle raw binary data such as varbinary or BLOB
* **``Video``**
  - Video data is growing at a similar pace to image data,image processing algorithms examine videos to detect everything from traffic congestion to intruders in the home.
  - Audio data,the resolution has a significant impact on storage a video consumes.Video duration is also another factor that impacts storage size.
* **``Large Text``**
  -oracle currently implents varchar as a synonym for varchar2

---
---
# CATEGORIES OF DATA
Semi-structured data represents the space between structured spreadsheets and unstructured videos

### Quantitative vs Qualitative Data
  - Quantitative data consists of numeric values,Data elements whose values come from counting or measuring are Quantitative
  - Quantitative data answers question like "How Many" and "How Much"
  - Qualitative data consists of frequent text values.Data elements whose values describe characteristics,traits and attitudes are all qualitative.

### DISCRETE VS CONTINUOUS DATA
- Numeric data comes in two different forms:discrete and continuous.
- Discrete data is data that represents measurements that can't be subdivided.discrete data is useful when you have things you want to count.
- It's typically represented with bar graphs where each bar represents a specific coun
- When you measure things like height and weight,you are collecting continuous data.
- While whole numbers represent discrete data ,continuous data typically need a decimal point.
- Qualitative data is discrete,but quantitative data can be either discrete or continuous data
- Discrete applies when counting while continuous applies when measuring.
- It's typically visualized with line graphs or histograms, where the line or bars connect smoothly to show the 
  range of possible values
### CATEGORICAL DATA
- Text data with a known ,finite number of categories is categorical,When considering an individual data element.it is possible to determine whether or not it is categorical
- Categorical data, also known as qualitative data, is a different breed compared to discrete and continuous 
  data. It doesn't deal with numbers representing quantities, but rather categorizes things based on qualities 
  or labels
- You can also use categories to enforce data validation when someone is first entering data.
- Category enforcement has the effect of improving data quality

### DIMENSIONAL DATA
- Dimensional modeling is an approach to arranging data to facilitate analysis
- Dimensional modeling organizes data into fact tables and dimension tables
- A table holding appointment data would be called a fact table
- Dimensions are tables that contain data about the fact
- Dimensional data contains groupings of individual attributes about a given subject.

## Common Data Structures
data needs to be stored in a consistent ,organized manner.

#### Structured Data
- Tabular data is structured data ,with values stored in a consistent ,defined manner,it is organized into columns and rows.
- Data is consistent when all entries in a column contain the same type of value(this particular method of organization facilitates aggregation)
- Structured data also make summarization easy.
- Structured data does not translate directly to data quality

#### UNSTRUCTURED DATA
Unstructured data is qualitative ,describing the characteristics of an event or an object.**Images ,phrases,audio or video recording and descriptive text.** are all examples of unstructured data.
* unstructured data also represents a significant opportunity.
* Machine data is a common source of unstructured data.Machine data has various sources including internet of things devices,smartphones,tablets,personal computers and servers
* The key-value concept underpins the design of object storage.The key becomes unique identifier and the value is the unstructured data itself
* To access the contents of a file ,you need to know its key

#### SEMI-STRUCTURED DATA
* is data that has structure and that is not tabular.
* Email is an example of semi structured formatting options.These formatting options use seprators or tags to 
  provide context aroud a data element

---
## Common File Formats

## Text Files
A text file is a basic type of computer file that stores information as plain text. This means it contains only letters, numbers, symbols, and some basic formatting characters like newline breaks
Text files are one of the the most commomly used data file formats.
* They consist of plain text and are limited in scope to alphanumeric data
* Reasons text files are so widely adopted is their ability to be opened regardless of platform or operating system without needing a proprietary piece of software.
* When machines generates data ,the output is commonly stored in a text file
* A unique character known as ``delimiter`` facilitates transmitting structured data via a text file,delimiter is the character that separates individual fields
* Delimiter can be any character
* when a file is comma-delimiited ,it is known as a **COMMA-SEPRATED VALUES**
* When a file is tab-delimited it is called a tab-separated values(TSV)

## Fixed-width Files
* Fixed width files are more laborious to create since they require a few extra steps

## JavaScript Object Notation
* Javascript Object Notation(JSON) is an open standard file format, designed to add structure to a text file 
  without incurring significant overhead.
* json is easily readable by people and easily parsed by modern programming languages
* Python ,R and Go have libraries containing functions that facilitate reading and writing JSON files.

## Extensible Markup Language (XML)
Is a markup language that facilitates structuring data in a text file
* XML incurs more overhead because it makes extensive use of tags. Tags describe a data element and enclose each value for each data element.
* Tags help readability ,they add a significant amount of overhead
* XML was the data format of choice and facilitated Asynchronous JavaScript and XML (Ajax) web development techniques.

## HyperText Makruo Language (HTML)
*  is a markup language for documents designed to be displayed in a web browser.
*  HTML pages serve as the foundation for how people interact with the World Wide Web. Similar to XML, HTML is a tag-based language.

  # Module 2 : Data preparation and Exploration

  # Chapter 3
- Data Concepts and Environments
- Identify basic concepts of data schemas and dimesions
- Understanding the domain of data Mining
- explain data acquisition concepts
- Explain common techniques for data manipulation and query optimization

  ## EXPLORING DATABASES
  * TWo categories
  * 1.Relational
  * 2.Nonrelational

  Databases fall into two categories,many of the systems we interact with daily produce tabular data .this is because tabular data is highly structured ,

### The Relational Model
* IBM's Edgar F.codd developed the relational model for database management,it builds on the concept of tabular data

# Relational Database
Relational databases are a fundamental type of database that store and organize data in a structured way. They follow a specific model based on tables and relationships between those tables, making data retrieval and manipulation efficient
![Screenshot 2024-04-11 092350](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/49f915d0-872c-4dcd-8f66-bf6c7fc799d9)
Relational Model:

Data is organized into tables (also called relations). Each table represents a specific entity or concept, like customers, orders, or products.
Tables consist of rows and columns. Rows represent individual records (instances of the entity), and columns represent attributes (specific characteristics of the entity). For instance, a Customer table might have rows for each customer and columns for attributes like name, address, and email.
* Understanding that the header corresponds to the name of an entity.
* Each row represents an individual attribute associated with a person.
* Each of these entities becomes a separate table in the database,with a column for each attribute
* Each row represents an instance of the entity
* The power of the relational model is that it also allows us to describe how entities connect or relate ,to each other

#### Entity Relationship Diagram(ERD)
* it is a visual artifact of the data modeling process.
* it shows  the connection between related entities
* A relationship is a connection between entities
* Cardinality refers to the relationship between two entities,showing how many instance of entity relate to instance in another entity.
* We specify cardinality in an ERD with various line endings
* 1st Component of the terminator indicates whether the relationship between two entities is optional or required
* 2nd Component indicates whether an entity instance in the first table is associated with a single entity instance in the related table or if an association can exist with multiple entity instances.

![Screenshot 2024-04-11 094114](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/0c7fbed9-089e-4f8b-8e7a-328f3bc60e2d).

* Unary Relationship is when an entity has a connection with itself
* Binary Relationship connects two entities
* Ternary Relationship connects three entities

Binary relationship are most common and easy to explore ,whereas unary and ternary are comparatively complex and rare
* The ability to read ERD's helps you understand the structure of a relational database
* ERDS are useful when formulating how to retrieve information from the data that is spread across multiple tables ,becausethe diagrams allow you to visualizw the connections between entities.

## RELATIONAL DATABASES
Relational databses are pieces of software that let you make an operational system out of an ERD.
* Start with relational model and create a physical design
* Relational entities correspond to database tables ,and entity attributes corresponds to table columns
* When creating a database table ,the ordering of columns does not matter because you can specify the column order when retrieving data from a table
* When an attribute becomes a column ,you assign it a data type
* Schema as an ERD with the additional details needed to create a database

![Screenshot 2024-04-11 100607](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/1a3dee67-18da-43b3-8b5f-7cf4256f986b)

* An associative table is both a table and a relationship.
* An associative table lets us identify the relationship between a specific animal and a particular person with a minimun amount of data duplication.

* A **``Primary key``** is one or more attributes that uniquely identify a specific row in a table
* Synthethis primary key is an attribute whose only purpose is to contain unique values for each row in the table
* Foreign key is one or more columns in one table that points to corresponding columns in a related table
* A foreign key reference another table's primary.
* Every row in a relational database must be unique
* To pull data from a relational database table,you perform a query,we compose queries using a programming language called **Structured Query Language(SQL)**

### Relational Database Providers
* Oracle is one of the most mature databse platforms
* Microsoft developed SQL  Server and the open source community created offerings including MySQL,MariaDB and PostgreSQL .Amazon Web Services (AWS) developed Aurora
* Aurora is compatible with MySQL and PostgreSQL,Aurora is unique because it takes advantage of AWS's underlying cloud platform and is easy to scale


## NONRELATIONAL DATABASES
a nonrelational database does not have a predefined structure based on tabular data.result is a highly flexible approach to storing data.the Data types available in relational databases are absent.

## EXAMPLES OF NONRELATIONAL DATABASE
* it includeas key-value,document,column family and graph.

 ### KEY-VALUE
 -  database is one of the simplest ways of storing data,data is stored as a collection of keys and their corresponding values
 - A key must be globally unique across the entire database.The use of keys differs from a relational database,where a given key identifies an individual row in a specific table
 - There are no structural limits on the values of a key.
 - A key can be a sequence of numbers,alphanumeric strings or some other combination of values,
 - Data that corresponds with the key can be any structured or unstructured data type
 -  One reason for choosing a key-value database is when you have lots of data and can search by a key's value.

### DOCUMENT

* A document database is similar to key-value database ,with additional restrictions in a key-value data,the 
  value can contain anything.
* with document database the value is restricted to a specific structured format
* With a known, structured format, document databases have additional flexibility beyond what is possible with key-value.
*  The document key is the profile name.

### Column Family
* Column-family databases use an index to identify data in groups of related columns.
* This design facilitates distributing data across multiple machines, which enables handling massive amounts of data
* The ability to handle large data volumes is due to the technical implementation details of how these databases organize and store
* column-family databases optimize performance when you need to examine the contents of a column across many rows. The main reason for choosing a column-family database is its ability to scale

### Graph
* Graph databases specialize in exploring relationships between pieces of data.
* Relational models focus on mapping the relationships between entities. Graph models map relationships between actual pieces of data.

# DATABASE USE CASES
transactional and reporting systems need different implementation approaches to serve the people who use them efficiently
* Databases tend to support two major categories of data processing:``Online Transactional Processing(OLTP)`` and `` Online Analytical processing(OLAP)``

## ONLINE TRANSACTIONAL PROCESSING

* OLTP  systems handle the transactions we encounter everyday,e.g transactions include booking a flight reservation,ordering something online.or executing  A STOCK TRADE
* While a number of transactions a system handles on a given day can be high,individual transactions process small amounts of data
* OLTP systems balance the ability to write and read data efficiently

# Normalization
Description = Normalization is a process for structuring a database in a way that minimizes duplication iof data

* one of the principles is that a given piece of data is stored once and only once,
* Normalized database is ideal for processing transactions.

### First normal form(1NF)
*  is when every row in a table is unique and every column contains a unique value.
![Screenshot 2024-04-11 133031](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/12c1185f-bfde-420f-ae0c-6d59caa1868f)

### Secod normal form(2NF)
starts when 1NF leaves off.in addition to each row being unique,2nf applies an additional rule stating that all nonprimary key values must depend on the entire primary key
* it eliminates partial dependencies:a partial dependency occurs when a non-key attribute ,which is an attribute not part of the primary key) it relies on only part of the primary ,not the entire key to determine its value
 ![Screenshot 2024-04-11 134152](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/2f1eaf41-5235-4085-95a6-85cfd59ac542).

### Third normal form(3NF)
builds upon 2NF by adding a rule stating all columns must depend on only the primary key.
* A table must be in 2NF (follows the rules of 1NF and eliminates partial dependencies).
It eliminates transitive dependencies. A transitive dependency occurs when a non-key attribute (attribute not part of the primary key) relies on another non-key attribute, which in turn depends on the primary key, to determine its value
* Databases in 3NF are said to be highly normalized

---

## ONLINE ANALYTICAL PROCESSING
OLAP systems focus on the ability of organzition to analyze data.While OLAP and OLTP databases need to balance transactional read and write performance,which results in highly normalize design.Typically, OLTP databases are in 3NF 
* Databases that power OLAP systems have a denormalized design.so instead of having data distributed across multiple tables,denormalization results in wider tables than those found in an OLTP database.
* it is more efficient for analytical queries to read large amounts of data for a singe table instead of incurring the cost of joining multiple tabes together.
* The greater the number of joins ,the more complex the query .The more complex the query ,the longer it takes to retrieve results
---
---

# Schema Concepts
* Design of a database schema depends on the purpose it serves
* transactional systems require highly normalized databaseswhereas a denormalized design is more appropriate for analytical systems
* ``Data warehouse`` is a database that aggregates data from many transactional systems for analytical purposes
* Transactional data may come from systems that power the human resources, sales ,marketing and product divisions
* A ``data warehouse`` facilitates analytics across the entire company.
* ``Data mart`` is a subset of a data warehouse,
* Data warehouses serve the entire organization and data marts focus on the needs of a particular department within the organization
* A data lake stores raw data in its native format instead of conforming to a relational database structure
* ``Data lake`` is more complex than a data warehouse or data mart ,and requires additional knowledge about the raw data to make it analytically useful
* Relational databsases enforces a structure that encapsulates business rules and business logic

#### DIFFERENCE BETWEEN DATABASES AND DATA WAREHOUSES
* Databases are designed for transactions , Data Warehouses are designed for analytics and reporting
* Databases data is fresh and detailed , Data warehouses data is refreshed periodically and summarized
* Databases work slowly for querying large amounts of data and can slow down transactional processes.Data Warehouses don't interfere with any processes and are generally faster

## Star Schema
is designed to facilitate analytical processing ,
* Star schemas are denormalized to improve read performance over large datasets
* At the centre of the star is a fact table
* Fact table stores numerical facts about a business.
  ![Screenshot 2024-04-11 150707](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/27675ca2-f1b5-4da9-89fb-129987927b68)

## Snowflake
Data warehouses often use snowflake schemas, since many different systems supply data to the warehouse.

## DIMENSIONALITY
it refers to the number of attributes a table has.The greater the number of attributes the higher the dimensianality
* A Dimension table provides additional context around data in fact tables.
* One dimension you will frequently encounter is time. It is necessary to answer questions about when something happened or when something was true.
---
---

# DATA ACQUISITION CONCEPTS

#### Integration
Data frim transactional systems flow into data warehouses and data marts for anlysis
* We need to retrieve ,reshape and insert data to move data between operational and analytical environments

  variety of methods to transfer data efficiently and effectively
  * Extract,Transform and Load (ETL)

1. **Extract** =  extract data from the source system and place it in a staging area,The goal of the extract phase is to move data from a relational database into a flat file as quickly as possible
2. **Transform** = Transform the data,goal is to reformat the data from its transactional structure to the data warehouse's analytical design
3. **Load** : The purpose of the load phase is to ensure data gets into the analytical systems as quickly as possible.
* data is extracted from a source database and loaded directly into the data warehouse.
* Once the extract and load phases are complete, the transformation phase gets underway.
* One key difference between ETL and ELT is the technical component performing the transformation.
* With ETL, the data transformation takes place external to a relational database, using a programming language like Python. ELT uses SQL and the power of a relational database to reformat the data
* ELT has an advantage in the speed with which data moves from the operational to the analytical database.

## Data Collection Methods
Augmenting data from your transactional systems with external data is an excellent way to improve the analytical capabilities of your organization,internal data is a good place to start when analyzing how to grow your business.
* To improve the accuracy of your analysis, you want to include data about the weather, tourism, and your competitors.

### 1. APPLICATION PROGRAMMING INTERFACES(APIs)
 Is a structured method for computer systems to exchange information,it provides a consistent interface to calling applications,regardless of the internal databsae structure
 * APIs can be transactional ,returning data as json objects
 * APIs provide a consistent interface to calling applications
 * APIs can be transactional,returning data as JSON objects
 * APIs can also facilitate bulk data extraction,returning CSV files.

### 2. WEB SERVICES
A web service is an API you can call via Hypertext Transfer Protocol(HTTP) ,the language of the world wide Web
* A While web service is an API ,an API does not have to be a web service.

### 3. Web Scraping
Web scraping  is the process of automatically extracting data from websites.
* if data exists in a structured format ,you can retrieve it programmatically ,Programmatic retrieval of data from a website is knwon as web scraping.
* We can use software bots to scrape data from a website
* Web scraper reads a web page similar to a browser, such as Chrome, Safari, or Edge
* Web Scrapers read and parse the HTML to extract the data the web pages contain
* Web scraper has to account for pagination to ensure that you are leaving data behind.
* Scraper must understand how many result pages exist and then iterate through them to harvest the data.

### 4. HUMAN - IN-THE LOOP

### 5. SURVEYS
* One way to collect data directly from your customers is by conducting a survey
* You can design surveys to achieve your data collection goals and your audience

---
---
## Working With Data
* To turn a database into an operational database ready to accept data,you use the **Data Definition Language (DDL)** components of SQL
* DDL lets you create ,modify and delete tables and other associated database objects
* To generate insights ,a productive analyst must be comfortable using the Data Manipulation Language(DML) capabilities of SQL to
* Insert ,Modify and retrieve information from databases
* DDL manages the structure of a database,DML manages the data in the database

### DATA MANIPULATION
* so when manipulating data ,one of four possible occurs
* 1. Create new data
  2. Read existing data
  3. update existing data
  4. Delete existing data
* **CRUD(Create,Read,Update,Delete)**
* SQL uses verbs to identify the type of activity a specific statement performs,and for each CRUD activity there is a corresponding DML verb
![Screenshot 2024-04-11 150707](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/fd4363d6-85af-4e10-bfab-9a0ea29315f2)
*
Basic structure of SQL query that reads from a database ,SELECT , FROM and WHERE are all reserved words that have specific meanings in SQL
![Screenshot 2024-04-15 092802](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/38acafb2-ed88-4b6a-9caf-7d5a69493a9d)

* SELECT clause identifies the columns from the table(s) that are retrieved
* FROM clause in a query identifies the sources of data,which is frequently a database table
* SELECT AND FROM clauses are required for a SQL statement to return.

### SQL CONSIDERATIONS
SQL are case-insensitive,howeve the case sensitivity of column names and values depend on the database configuration

### FILTERING 
Filtering is a way to reduce the data down to only the rows that you need
* To filter data ,you add a WHERE clause to a query,but the column you are filtering on does not have to appear in the SELECT clause

SELECT Animal_Name ,Breed_Name

From Animal

Where Animal_Type = 'Dog'

### FILTERING AND LOGICAL OPERATORS
We need to use a logical operator to account for complex filtering needs
* We can enhance the query using the AND logical operator

SELECT Animal_Name.Breed_Name

FROM Animal

WHERE Animal_Type = 'Dog'

AND Weight > 60

*A AND operators evaluates the Animal_Type and Weight filters together
* OR is also used E.G we want to see the name and breed for all dogs and any animal that weigh more than 10 pounds regardless of the animal type

SELECT Animal_Name , Breed_Name

FROM Animal

WHERE  Animal_Type = 'Dog'

OR Weight > 10

### SORTING
* ORDER BY clause is component that makes sorting possible.it is similar to WHERE clause performs,you don't have to specify the colums you are using to sort the data in the SELECT clause
* ASC orderBy clause sorts in ascending order
* DESC with Order By sorts in descending order

### Date Functions 
* date columns are frequently found in OLAP environments
* Date columns also appear in transactional systems
* Storing date information about an event facilitates analysis across time
* The most important thing to note is that you have to understand the database platform you are using and how that platform handles dates and times

### Logical Functions
* Logical functions can make data substitutions when retrieving data
* Data in the underlying tables do not change when SELECT runs

IFF function follows tge syntax:
* IFF(boolean_expression,true_value,false_value)

1. Boolean Expression : The expression must return either TRUE or FALSE
2. TRUE Value : If the Boolean expression returns TRUE, the IFF function will return this value
3. False Value : If the Boolean expression returns FALSE, the IFF function will return this value.

## AGGREGATE FUNCTIONS
*  Aggregate functions summarize a query's data and return a single value.

![Screenshot 2024-04-15 092704](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/1285b404-03aa-4cef-9a10-fb1a27d6d48b)

### SYSTEM FUNCTIONS

* current data is a component of transactional records and enables time-based analysis in the future
* Current data is also necessary for a systems that uses an effective data approach.
* A database session begins when a person/program connects to a database.

### Query Optimization

##### PARAMETRIZATION
* Parsing translates the human-readable SQL into code the database understands.
* Parsing takes time and impacts how long it takes for a query to return data.
* Effective use of parameterization reduces the number of times the databases has to parse individual queries.

##### INDEXING
* When retrieving data from a table, the database has to scan each row until it finds the ones that match the filters in the WHERE clause.
* To speed up query performance, you need a database index.
*  A database index works like the index in the back of a book. Instead of looking at each page in a book to find what you are looking for,you can find a specific page number in the index and then go to that page
* database index can point to a single column or multiple columns. 

---
---
# CHAPTER 4 
## DATA QUALITY

### DATA QUALITY CHALLENGES

#### DUPLICATE DATA
It occurs when data representing the same transaction is accidentally duplicated within a system
* one common way of stopping duplicate data before it enters the system is to have a visual warning to alert
* Having multiple data sources for the same data elements is also a source of duplicate data.

#### Redundant Data
* Redundant data happens when the same data elements exist in multiple places within a system
* Data redundancy is a function of integrating multiple systems
* Ways to resolve redundant data,one approach is to synchronize changes to shared data elements between the Accounting and sales systems.
* The ETL logic ensures that the warehouses contains correct values
* Another Root cause of data redundancy is an inappropriate database design

#### Missing Values
* Missing values occur when you expect an attribute to contain data but nothing is there
* Missing values are known as null values,A null value is the absence of a value.
* A null is not a space,blank,or other character
* When a column optionally contains data,it is nullable ,meaning the column can contain null values
* Null values present several challenges depending on the tools you use to analyze data
* To handle missing values,we must first check for their existence.
* SQL offers functions to check for null and functions that can replace a null with a user-specified value.

#### INVALID DATA
invalid data are values outside the valid range for a given attribute
* Invalid value violates a business rule instead of having an incorrect data type
* Wehave to understand the context of a system to determine whether or not a value is invalid
* Text data is more complex,one thing that leads to invalid character data is an absence of referential integrity within a database
* if two tables have a relationship but no foreign keys,the conditions for invalid character data exist.Implementing relationships appropriately reduces the likelihood of invalid character data
* implementing referential integrity is an excellnt way to improve data quality.

#### NONPARAMETRIC DATA

Nonparametric data is data collected from categorical variables ,the rank order of the values is of significance ,not the individual values themselves

#### DATA OUTLIERS

A data outlier is a value that differs significantly from other observations in a dataset
* Outliers exist regardless of data type

#### SPECIFICATION MISMATCH

Specification describes the target value for a component,A specification mismatch occurs when an individual component's characteristics are beyond the range of acceptable values
* When data is invalid ,it has values that fall outside a given range,Specification mismatch occurs when data does not conform to its destination data type
* if Destination column is numeric and you have text data ,to resolve mismatch you must validate that the inbound data consistently maps to its target data type

#### DATA TYPE VALIDATION
* Data type validation ensures that values in a datasets have a consistent data type
* How the load process handles the data type validation failure determines whether or not the remaining rows load successfully.Depending on the tool ,a single failure may cause the load process to stop
* Programming languages ,SQL ,python and R all have data type validation functions.
* Use these functions to validate the data type for each column in a data file before attempting a database load
* it is the best interest to detect and remediate data type issues as early as possible to ensure data is ready for analysis

# Data Manipulation Techniques

## Recoding Data
* Recoding data is a technique you can use to map original values for a variable into anew values facilitate analysis
* Recoding groups data into multiple categories ,creating a categorical variable
* A categorical variable is either nominal or ordinal.
* Nominal variables are any variables with two or more categories where there is no natural order of the categories
* Oridinal cariables are categories with an inherant rank.

## Derived Variables
* A derived variable is a new variable resulting from a calculation on an existing variable
* Derived variables don't have to be categorical

## DATA Merge
it uses a common variable to combine multiple datasets with different structures into a single datasets
* Merging data improves data quality by adding new variables to your existing data
* Additional variables make for a richer dataset ,this positively impacts the quality of your analysis
* ETL processes commonly append data while transforming data for use in analytical environment.
* Since a data merge adds columns to a dataset, merging gives you additional data about a specific observation.

## Data Blending 
Data blending combines multiple sources of data into a single dataset at the reporting layer.While data bl;ending is conceptually similar to extract ,transform and load process
* Data Blending differs from ETL in that it allows an analyst to combine datasets in an ad hoc manner without saving the blended datasets in a relational database.

## Concatenation
Concatenation is the merging of separate variables into a single variable. Concatenation is a highly effective technique when dealing with a source system that stores components of a single variable in multiple columns.
* Concatenation frequently occurs when dealing with date and time data.
* Concatenation is also useful when generating address information.

## Data Append
A data append combines multiple data sources with the same structure, resulting in a new dataset containing all the rows from the original datasets. 
* When appending data, you save the result as a new dataset for ongoing analysis.

## Imputation
Imputation is a technique for dealing with missing values by replacing them with substitutes. 
* Merging multiple data sources, you may end up with a dataset with many nulls in a given column.
  
### Approaches an analyst can use for imputing values:

1. **``Remove Missing Data``**: with this approach ,you can remove rows with missing values without impacting the quality of your overall analysis
2. **``Replace With Zero``**: we replace missing values with a zero ,whether or not it is appropriate to replace missing data with a zero is contextual.
3. **``Replace with overall``** Average: instead of using a zero ,you can compute average weight value for all rows that have data and then replace the missing weight values with that calculated average.
4. **``Replace with Most Frequent (mode)``**: we take the most frequently occuring value ,called the mode ,and use that as the constant
5. **``Closest Value Average``** : with this approach you use values from the rows before and after the missing values

## Reduction
So when dealing with big data .it is frequently unfeasible and inefficient to manipulate the entire dataset during analysis
* **Reduction** is the process of shrinking an extensive dataset without negatively impacting its analytical value
* ``Dimensionality reduction`` and ``numerosity reduction`` are two techniques for data reduction.

**``DIMESIONALITY REDUCTION``**
  removes attributes from a dataset.Removing attributes reduces the dataset's overall size

**``NUMEROSITY REDUCTION``**
it reduces the overall volume of data,numerosity reduction can improve the efficiency of your analysis
* one way to reduce the volume of quantitative data is by creating a histogram.
* Histogram is a diagram made up of rectangles or bars that shows how frequently a specific value occurs

Another approach to reduce data is through sampling,sampling is a technique that selects a subset of individual records from the initial dataset


### 1. Aggregation
is the summarization of raw data for analysis .Aggregating data provides answers that help make decision .Aggregation also means controlling privacy.
### 2. Transposition
transposing data is when you want to turn rows into columns or columns into rows to facilitate analysis.
### 3. Normalization 
normalizing data converts data from different scales to the same scale ,if one wants to compare whose measurements use different units,you want to normalize the data,after normalization the dataset is ready for statistical analysis
### 4 Min-Max Normalization
![Screenshot 2024-04-16 100945](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/79a2a0fc-fa8a-4a74-bfe1-fc5343ee1c94)
![Screenshot 2024-04-16 101028](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/c3cd2838-42fd-4561-86f4-2b67dbf87ada)

### Parsing/String Manipulation
Raw data can contain columns with composite or distributed structural issues.
A composite issue is when a raw data source has multiple, distinct values combined within a single character column.
Composite columns need to be split into their component parts to aid analysis.

# Managing Data Quality
as an analyst you must recognize scenarios that create he conditions for data quality issues.

### Circumstances to check for Quality.
* Errors during data **acquisition**, **transformation**, **manipulation**, and **visualization** all contribute to degrading data quality
* **``Data acquisition``**
* **``Data Transformation and conversion``**
* **``Data Manipulation``**
* **``Final product preparation``**

### DATA QUALITY DIMENSIONS
six dimensions to take into account when assessing data quality are accuracy ,completeness ,consistency ,timeliness ,uniqueness and validity .By understanding these dimensions and how they are related will help improve data quality.

 1. **``Data Accuracy``** = refers to how closely data reflects the real world. For example, an accurate customer address would allow a company to send products to the correct location.
  2. **``Data copmpleteness``** = refers to whether all of the necessary data is present. For example, a complete customer record would include the customer's name, address, and phone number.
  3. **``Consistency``** =  refers to whether data is consistent within itself and across different systems. For example, a customer's name should be spelled the same way in all of the company's databases.
  4. **``Data Timeliness``** = refers to how up-to-date data is. For example, a company's financial data should be timely so that it can be used to make informed business decisions.
  5. **``Data Uniqueness``** = refers to whether each record in a data set is unique and can be easily identified. For example, each customer should have a unique customer ID.
  6. **``Validity``**=  refers to whether data meets the defined standards for its format and content. For example, a customer's email address should be in a valid format.

### Data Quality Rules and Metrics
* When consolidating data from multiple source systems into an analytics environment, one factor you want to assess is the conformity or nonconformity of data.
* if source data does not match the destination data type size and format ,you have nonconformity.
* The Warehouse Load ETL needs to ensure consistency as it propagates data from these source systems into the Data Warehouse to ensure data quality.
* This nonconformity presents an ETL challenge.
* One way to validate data conformity issues is to confirm how many rows pass successfully to the target environment and how many fail
* . A data engineer then resolves the root cause of the data quality issue before sending the remediated data into the Data Warehouse.
*  the nonconformity of a single row does not cause the entire load process to fail.

### Methods To Validate Quality

1. **Reasonable Expectation:**  This refers to setting realistic goals for data quality based on its intended use.  Not all data needs to be absolutely perfect.  For example, mailing list data might have a slightly higher tolerance for duplicates compared to financial transaction data. 

2. **Data Profiling:** This involves analyzing data to understand its statistical properties.  Data profiling tools summarize key characteristics like:
    * **Value Distribution:** How data points are spread across the spectrum (e.g., most customers are between 25-45 years old).
    * **Presence of Duplicates:** Identifying and quantifying records that appear identical.
    * **Identification of Outliers:**  Values that fall far outside the expected range (e.g., a sale of $1 million when typical sales are under $1000).

Data profiling helps identify potential issues with accuracy, completeness, and consistency.

3. **Data Audits:**  This is a systematic review of data quality controls and procedures.  Data audits assess:
    * **Data Collection Methods:**  Are there proper procedures for gathering data? 
    * **Data Storage:**  Is data stored securely and with limited access?
    * **Data Documentation:**  Is data clearly defined with understandable formats and standards?
    * **Data Security:**  Are there safeguards in place to protect data from unauthorized access or modification?

Data audits ensure data is managed effectively and meets compliance requirements.

4. **Sampling:** This involves selecting a subset of data from a larger dataset to assess its quality.  Sampling is useful when dealing with very large datasets as analyzing everything might be impractical.  There are different sampling techniques,  like random sampling (selecting data points at random) or stratified sampling (ensuring the sample reflects the proportions of the entire dataset).

By analyzing a representative sample, you can gain insights into the overall quality of the data.

5. **Cross Validation:** This is a statistical technique used to evaluate the performance of a model.  It involves splitting the data into two sets: a training set used to build the model and a testing set used to assess its accuracy on unseen data.  Cross-validation helps ensure the model generalizes well and doesn't simply memorize the training data.

## EXAM PREPARATION 
### Describe the unique challenge of missing data values. 

A missing value is the absence of a value. Regardless of the programming language you use to manipulate data, you need additional checks to account for the lack of a value. You cannot compare values to the absence of a value. Instead, you first have to determine whether or not a value exists before doing the comparison.

### Describe why it is crucial to account for data outliers. 

An outlier is an observation whose value differs significantly from other observations of the same type. Leaving outliers in your data can negatively impact the quality of your analysis.

### Describe the difference between data merging and data blending. 

Both data merging and data blending combine data sources. However, data merging combines sources programmatically, typically through an ETL operation. Data blending is when an analyst merges data temporarily while exploring or visualizing data.

### Differentiate between dimensionality and numerosity reduction. 

Dimensionality reduction is a technique for removing attributes that are not relevant for the analysis at hand. Numerosity reduction is a technique for reducing the overall size of a dataset to facilitate processing efficiency.

### Describe how you can enforce data validity. 

Data validity is the data quality dimension that identifies whether a given value falls within an expected range. Combining referential integrity in the database with data type validation is a layered approach to ensuring only valid data gets into a system.

---
---
# Chapter 5 
## Data Analysis and Statistics

Descriptive stattistics is a branch of statistics that summarizes and describes data. You use descriptive statistics as measures to help you understand the characteristics of your dataset
*You also use descriptive measures to develop summary information about all of a variable's observations.

### Measures of Frequency
it helps ou understand how often something happens.so when encountering a dataset you want to determine how much data you are working with in order to help guide your analysis

1. ``Count``= count the number of observations and understanding the total number of observations is a frequently performed task
![Screenshot 2024-04-16 132818](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/dc52cb44-bc03-423e-93fb-758a42563684)
2.``Percentage`` = The percentage is a frequency measure that identifies the proportion of a given value for a variable with respect to the total number of rows in the dataset,calculate a percentage, you need the total number of observations and the total number of observations for a specific value of a variable.
3. ``Frequency`` = Frequency describes how often a specific value for a variable occurs in a dataset. typically explore frequency when conducting univariate analysis.

### Measures of Central Tendency
help establish an overall perspective on a given dataset,use measures of central tendency to identify the central, or most typical, value in a dataset.

1. **``Mean``** = mean, or average, is a measurement of central tendency that computes the arithmetic average for a given set of numeric values. To calculate the mean, you take the sum of all values for an observation and divide by the number of observations,In the comparatively unlikely event that you have a complete census for your population
![Screenshot 2024-04-16 134745](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/b9e80f1c-4caa-4ab5-ab93-843fcc9a8343)
2. **``Median``** median, which identifies the midpoint value for all observations of a variable. The first step to calculating the median is sorting your data numerically.
  ![Screenshot 2024-04-16 135000](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/5701fc01-dc9b-413a-aaac-d6f6735b7be0)
3. **``Mode``** = The mode is a variable's most frequently occurring observation.

### Measures of Dispersion
measures of dispersion to create context around data spread.                                             
1.**``Range``**=range of a variable is the difference between its maximum and minimum values.Understanding the range helps put the data you are looking at into context and can help you determine what to do with outlier values.

2. **``Distribution``** = a probability distribution, or distribution, is a function that illustrates probable values for a variable, and the frequency with which they occur,Histograms are an effective tool to visualize a distribution, because the shape provides additional insight into your data and how to proceed with analysis.

3. **``Normal distribution``** = is symmetrically dispersed around its mean, which gives it a distinctive bell-like shape. Due to its shape, the normal distribution is also known as a “bell curve.”, normal distribution is applicable across a number of disciplines due to the central limit theorem (CLT), a foundational theorem for statistical analysis.
   
4. **``Skewed Distribution``**=skewed distribution has an asymmetrical shape, with a single peak and a long tail on one side. Skewed distributions have either a right (positive) or left (negative) skew.
   -  right, the mean is typically greater than the median
   -  left , the mean i typically less than the median

5. **``Bimodal Distribution``** =bimodal distribution has two distinct modes, whereas a multimodal distribution has multiple distinct modes. When you visualize a bimodal distribution, you see two separate peaks.

6. **``Variance``**=Variance is a measure of dispersion that takes the values for each observation in a dataset and calculates how far away they are from the mean value. dispersion measure indicates how spread out the data is in squared units. Mathematically,  signifies population variance, which you calculate by taking the average squared deviation of each value from the mean,
![Screenshot 2024-04-16 142232](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/16df62a9-431b-4325-b12e-434ab8cf203c)
![Screenshot 2024-04-16 142245](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/9b0354a3-bb82-4c0a-8710-6698ad6bb42d)

7. **``Standard Deviation``**=Standard deviation is a statistic that measures dispersion in terms of how far values of a variable are from its mean.Specifically, standard deviation is the average deviation between individual values and the mean. Mathematically,  signifies population standard deviation, which you calculate by taking the square root of the variance
![Screenshot 2024-04-16 142858hnmdgf](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/bf482efc-dde4-4ded-85f9-65f22519ad9a)
x![Screenshot 2024-04-16 142914](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/de9922cc-02e7-4834-9a81-f0b135d4d580)

* calculating variance is an important step on the way to determining standard deviation. Similar to variance, the standard deviation is a measure of volatility, with a low value implying stability.

### Each Sample is Unique

1. **``Special Normal Distributions``**= Central Limit Theorem and empirical rule combine to make the normal distribution the most important distribution in statistics.
2. **``Standard Normal Distribution``** =  standard normal distribution, or Z-distribution, is a special normal distribution with a mean of 0 and a standard deviation of 1.
3. **``Student's T-Distribution``**=Student's t-distribution, commonly known as the t-distribution, is similar to the standard normal distribution in that it has a mean of 0 with a bell-like shape. One way the t-distribution differs from the standard normal distribution is how thick the tails are since you can use the t-distribution for sample sizes of less than 30

## Measure of Positions
Understanding a specific value for a variable relative to the other values for that variable gives you an indication of the organization of your data.
* The process of obtaining quartiles is similar to that of determining the median. You first sort a numeric dataset from smallest to largest and divide it positionally into four equal groups. Each grouping is known as a quartile.
* The first quartile is the group that starts with the minimum value, whereas the fourth quartile is the group that ends with the maximum value.

---
# INFERENTIAL sTATISTICS
Is a branch of statistics that uses sample data to draw conclusion about the overall population.

### Confidence Intervals
* when you take a sample from a population ,the statistics you generate are unique to the sample, in order to make inference about the population as a whole.
* Confidence intervals describes the possibility that a sample statistic contains the true population parameter in a ange of values around the mean
* When calculating a confidence interval you end up with a lower bound value and an upper bound value
* So Given the confidence interval range ,the lower bound is the lower limit ,and the upper bound is the upper limit

### Confidence interval Considerations
* When calculating a confidence interval ,you need to specify the confidence level in addition to the sample mean,population ,standard deviation and sample size
* Based on the empirical rule ,the confidence level is a percentage that describes the range around the mean.
*  The wider the confidence level, the more confident one can be in capturing the true mean for the sample. High confidence levels have a wide confidence interval, while low confidence levels have a narrower confidence interval.
*  The critical value is a Z-score you specify to denote the percentage of values you want the confidence interval to include
*  The standard error measures the standard deviation of the distribution of means for a given sample size.
![Screenshot 2024-04-17 094435](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/7ddcb97a-c33d-4e38-9d31-4fc0a348a220)

### Substituting Sample Standard Deviation For Population Standard Deviation.
* When calculating confidence intervals, you need to have the standard deviation of the entire population.
*  since getting measures about the whole population is challenging, the population standard deviation is likely unknown. In that case, while it's more precise to use the t-distribution.

![Screenshot 2024-04-17 094911](https://github.com/BafanaMadume/Data-analyticsweek1/assets/141032267/05bb6d1b-4867-475f-a336-a32ce9b76131)

### Hypothesis Testing 
 A hypothesis test consists of two statements, only one of which can be true. It uses statistical analysis of observable data to determine which of the two statements is most likely to be true.
* A hypothesis test consists of two components: the null hypothesis and the alternative hypothesis.
1. first develop the null hypothesis. A null hypothesis (H0) presumes that there is no effect on the test you are conducting.
2. When hypothesis testing, your default assumption is that the null hypothesis is valid and that you have to have evidence to reject it.
3. Develop the alternative hypothesis. The alternative hypothesis (Ha) presumes that the test you are conducting has an effect.
4. To determine the statistical significance of whether to accept or reject the null hypothesis, you need to compare a test statistic against a critical value.
5. A **``test statistic``** is a single numeric value that describes how closely your sample data matches the distribution of data under the null hypothesis.
6. In order to get the right critical value, you need to choose a significance level. A significance level, also known as alpha (a), is the probability of rejecting the null hypothesis when it is true.
**a = 100% - C**

## Simple Linear Regression 
* ``Simple linear regression`` is an analysis technique that explores the relationship between an independent variable and a dependent variable.
* Linear regresson to ifdentify whether the independent variable is a good predictor of the dependent variable
* Using simple linear regression, you want to determine if the dependent variable truly depends on the independent variable.

# Analysis Techniques

### Determine Type of Analysis
*  The first step to understanding the objectives is to ensure that you have clarity on the business questions at hand.
* goal of answering a business question is to develop an insight that informs a business decision

### TYPES OF ANALYSIS
* Trend analysis is one of the type of analysis.
* Trend analysis seeks to identify patterns by comparing data over time.
* Performance analysis examines defined goals and measures performance against then and can inform the development of future projections

### Exploratory Data Analysis
Exploratory data analysis (EDA) uses descriptive statistics to summarize the main characteristics of a dataset, identify outliers, and give you context for further analysis.

* Approaches to conducting an EDA,they encompass the following steps
These all represent different steps you might take during the process of Exploratory Data Analysis (EDA) to get a good understanding of your data's characteristics and quality. 

**1. Check Data Structure:**

* This involves examining the organization of your data. 
* Key aspects to consider include:
    * **Data Types:**  Understanding the data types of each variable (numerical, categorical, text, etc.) is crucial for choosing appropriate analysis techniques.
    * **Missing Values:** Identifying how missing data is handled (e.g., completely missing, filled with placeholder values) and assessing the potential impact on your analysis.
    * **Presence of Duplicates:**  Checking for duplicate records and determining if they need to be removed or addressed depending on your specific analysis.

**2. Check Data Representation:**

* This refers to how the data is formatted and labeled. 
* Here's what to look for:
    * **Variable Names:**  Meaningful and consistent variable names enhance readability and understanding of the data.
    * **Value Labels:**  For categorical data, ensure category labels are clear and consistent.  
    * **Units:**  Verify that units of measurement are consistent throughout the data set (e.g., meters vs centimeters).

**3. Identify Outliers:**

* Outliers are data points that fall far outside the expected range of the majority of the data. 
* Techniques to identify outliers include:
    * **Boxplots:**  Visually identify data points that fall outside the interquartile range (IQR).
    * **Statistical Tests:**  Use statistical tests like standard deviation or interquartile range to define outliers mathematically.

It's important to understand the cause of outliers and determine if they are errors or valid data points that need to be considered in your analysis.

**4. Summarize Statistics:**

* This involves calculating key statistical measures to get a sense of the central tendency, spread, and distribution of your data. 
* Common summary statistics include:
    * **Central Tendency:** Mean, Median, Mode - which represent the "average" value
    * **Spread:**  Standard Deviation, Variance, Interquartile Range (IQR) - which indicate how data points are distributed around the central tendency
    * **Distribution Skewness:**  Measures if the data is symmetrical or skewed towards one side.

These statistics provide a quick overview of the data's characteristics.

**5. Check Assumptions:**

* Many statistical models and techniques rely on certain assumptions about the underlying data. 
*  For example, linear regression assumes normality (data is distributed in a bell-shaped curve).
*  It's crucial to check these assumptions to ensure the chosen analysis method is appropriate for your data. 

### Exam Preparation 
**Differentiate between descriptive and inferential statistics.**

Descriptive statistics help you understand past events by summarizing data and include measures of frequency and measures of dispersion. Inferential statistics use the powerful concept of concluding an overall population using a sample from that population.

**Calculate measures of central tendency.**

Given a dataset, you should feel comfortable calculating the mean, median, and mode. Recall that the mean is the mathematical average. The median is the value that separates the lower and higher portions of an ordered set of numbers. The mode is the value that occurs most frequently. While mean and median are applicable for numeric data, evaluating the mode is particularly useful when describing categorical data.

**Explain how to interpret a p-value when hypothesis testing.**

Recall that p-values denote the probability that a test statistic is as extreme as the actual result, presuming the null hypothesis is true. The lower the p-value, the more evidence there is to reject the null hypothesis. Generally speaking, it is safe to consider p-values under 0.05 as being statistically significant. With p-values greater than 0.05, there is less evidence supporting the alternative hypothesis.

**Explain the difference between a Type I and Type II error.**

When hypothesis testing, a Type I error is a false positive, while a Type II error is a false negative. Suppose you have a null hypothesis stating that a new vaccine is ineffective and an alternative hypothesis stating that the vaccine has its intended impact. Concluding that the vaccine is effective when it isn't is a Type I error. A Type II error is a false conclusion that the vaccine does not work when it does have the intended effect.

**Describe the purpose of exploratory data analysis (EDA).**

One of the first things you should perform with any new dataset is EDA, a structured approach using descriptive statistics to summarize the characteristics of a dataset, identify any outliers, and help you develop your plan for further analysis

---
---
# MODULE THREE 

## Chapter 6 : DATA ANALYTICS TOOLS

### SPREADSHEETS
Spreadsheets provide intuitive way to organize our data into rows and columns
* Spreadsheets are productivity software packages that allows user to create documents that organize any type of data into rows and columns
* So users may place any data they like in the spreadsheet and then quickly easily perform mathematical calculations ,e.g finding sum of values in a row or searching out the minimun ,maximum ,mean and median values in a dataset
* Spreadsheets lack any of the constraints of a relational database , While you can certainly organize data in a spreadsheet, there's no requirement that you do so. If you'd like, you can mix numbers, text, dates, and other data elements all in the same column.
* Power of spreadsheets comes from the fact that virtually anybody can use one,The barrier to entry is low because they're readily accessible and easy to use.
* Microsoft Excel is the most commonly used desktop spreadsheet application.Excel then allows users to perform calculations and visualizations on their data

### Programming Languages
Business analysts and data scientists need a way to be able to load, manipulate, and analyze data outside of the constraints of software written by another organization.

#### R
R is a programming language is extremely popular among data analysts because it is focused on creating analytics application.
* R is available to everyone as a free,open-source language developed by a community developers.
* R also continues to grow in popularity because of its adoption by the creators of machine learning methods.
*  new machine learning technique created today quickly becomes available to R users in a redistributable package, offered as open-source code on the Comprehensive R Archive Network (CRAN), a worldwide repository of popular R code.
*  Modern R developers choose to write, test, and deploy their code using an integrated development environment (IDE) called **RStudio**

#### PYTHON
Python is about the same age as R, but the major difference between Python and R is that Python is a general-purpose programming language. 
* Python also has specialized libraries that focus on the needs of analysts and data scientists.
* Python Data Analysis Library (pandas) provides a set of tools for structuring and analyzing data.

#### Structured Query Language (SQL)
Structured Query Language (SQL) is the language of databases.
* developer, administrator, or end user interacts with a database, that interaction happens through the use of a SQL command. SQL is divided into two major sublanguages:

1. **DATA DEFINITION LANGUAGE (DDL)** =  used by developers and administrators,its used to define the structure of the database itself.it doesn't work with the data inside a database,but it sets the ground rules for the database to function
2. **DATA MANIPULATION LANGUAGE (DML)** = IS THE SUBSET OF SQL commands that are used to work with the data inside of a database,they do not change the database structure ,but they add,remove and change the data inside a database.

###### Three DDL Commands 
1 . **``CREATE``** = command is used to create a new table within your database or a new database on your server
2. **``ALTER``** = command is used to change the structure of a table that you've already created ,if you want to modift your database or table ,the ALTER command lets you make those modifications.
3. **``DROP``** = command deletes an entire table or database from your server ,must be used with caution.

##### Four DML Commands 
1. **``SELECT``** = command is used to **retrieve information from a database**. It is the most commonly used command in SQL as it is used to pose queries to the database and retrieve the data that you're interested in working with.
2. **``INSERT``** = command is used to add new records to a database table .If you are adding a new employee, customer order, or marketing activity, the INSERT command allows you to add one or more rows to your database.
3. **``UPDATE``** = command is used to modify rows in the database. If you need to change something that is already stored in your database, the UPDATE command will do that
4. **``DELETE``** =  command is used to delete rows from a database table. The DROP command deletes an entire database table, whereas the DELETE command just deletes certain rows from the table.

 developer, administrator, or power user who knows SQL might directly access the database server and send it a SQL command for execution.  happens through a graphical user interface, such as the Azure Data Studio.
 * Utilities like Azure Data Studio can do more than just retrieve data. They also offer a graphical way for database administrators to reconfigure a database.

## STATISTICS PACKAGES
Software packages may be used by anyone interested in data analysis .

### IBM SPSS
popular pieces of statisticaal software ,IBM's SPSS package,SPSS is one of the oldest statistical software packages released in 1968
IBM SPSS is a powerful statistical software suite developed by IBM that is used for data management, advanced analytics,  business intelligence, and even criminal investigation.  It  was originally created by SPSS Inc. and acquired by IBM in 2009. [Image of IBM SPSS logo]

SPSS stands for Statistical Package for the Social Sciences, reflecting its original focus on social science research. However, it has become a widely used tool across various industries due to its versatility and functionality.

Here are some key features of IBM SPSS:

* **Data Management:** Powerful tools for data cleaning, transformation, and manipulation to prepare your data for analysis.
* **Statistical Analysis:** A vast library of statistical tests and procedures, from basic descriptive statistics to complex multivariate analysis.
* **Data Visualization:** Create charts and graphs to explore data patterns and relationships.
* **Machine Learning:**  SPSS Modeler offers features for building and deploying predictive models.
* **Text Analytics:** Analyze text data to uncover insights from surveys, social media, and other sources.
* **Open-Source Integration:** Integrate with open-source  programming languages like R and Python for even more advanced capabilities.

SPSS offers a user-friendly interface with drag-and-drop functionality and clear menus, making it accessible to users with varying levels of statistical expertise. It also provides extensive documentation and tutorials to help you get started and learn more advanced techniques.

Here are some of the benefits of using IBM SPSS:

* **Improved Decision Making:**  Data-driven insights to support better decision making across your organization.
* **Increased Efficiency:** Streamline data analysis processes and save time.
* **Enhanced Research Capabilities:** Powerful tools for researchers to explore complex relationships and test hypotheses.
* **Reduced Costs:** Identify areas for improvement and optimize processes to reduce costs.

Overall, IBM SPSS is a valuable tool for anyone who needs to work with data and extract meaningful insights. It is a comprehensive and user-friendly platform that can be used for a wide range of data analysis tasks.

### STATA
Stata offers same features as SPSS and SAS and providdes users with both a graphical interface and a command-line interface depending on their personal preference. Stata is a statistical software package developed by StataCorp for data manipulation, visualization, statistics, and automated reporting. It is commonly used by researchers in many fields, including biomedicine, economics, epidemiology, and sociology. 

Stata is known for its powerful command-line interface, which offers a flexible and efficient way to perform complex data analysis tasks. It also has a graphical user interface (GUI) that provides a more user-friendly option for beginners.

Here are some of the key features of Stata:

* **Data Management:** Powerful tools for data cleaning, transformation, and manipulation to prepare your data for analysis.
* **Statistical Analysis:** A vast library of statistical tests and procedures, from basic descriptive statistics to complex multivariate analysis.
* **Data Visualization:** Create publication-quality charts and graphs to explore data patterns and relationships.
* **Programming Language:** Stata's command-line interface offers a sophisticated programming language for automating tasks, extending functionality, and developing custom analyses.
* **Replication and Reproducibility:** Stata facilitates reproducible research by providing tools for documenting and sharing analysis procedures.

Stata offers a variety of versions to suit different needs and budgets. Stata/IC is the basic version, while Stata/SE adds more advanced statistical features and Stata/MP supports parallel processing for faster analysis on multi-core computers.

Here are some of the benefits of using Stata:

* **Powerful and Flexible:** Stata's command-line interface and programming language provide a powerful and flexible environment for data analysis.
* **Comprehensive Statistical Features:** Stata offers a wide range of statistical procedures, making it suitable for a broad spectrum of research questions.
* **Publication-Quality Graphics:** Stata's graphics are known for their high quality and professional appearance, making them ideal for presentations and publications.
* **Strong Community Support:** Stata has a large and active community of users and developers who provide support and share resources.

Overall, Stata is a versatile and powerful statistical software package that is well-suited for a wide range of data analysis tasks. It is particularly popular among researchers who appreciate its flexibility, power, and ability to produce high-quality graphics.

### MINITAB
Minitab is another statistical software program alongside IBM SPSS and Stata that you might consider for your data analysis needs. Here's a breakdown of Minitab's functionalities and how it compares to the others:

**Minitab Overview:**

Minitab is a software package designed for data analysis, statistical modeling, and process improvement. It is particularly popular in Six Sigma quality improvement methodologies and widely used in various industries, including manufacturing, healthcare, and education. [Image of Minitab software logo]

**Key Features:**

* **Data Analysis:** Offers a comprehensive set of statistical tests for exploring data patterns, identifying relationships, and testing hypotheses. 
* **Six Sigma:** Provides tools for implementing Six Sigma methodologies, including defect analysis, process capability analysis, and control charts.
* **Design of Experiments (DOE):**  Helps design and analyze experiments to optimize processes and identify key factors influencing outcomes.
* **Data Visualization:** Creates clear and informative charts and graphs to communicate data insights effectively.
* **Ease of Use:**  Minitab offers a relatively user-friendly interface with menus and dialog boxes, making it accessible to users with a basic understanding of statistics.

**Minitab vs. Competitors:**

* **SPSS:** Both offer a wide range of statistical procedures, but SPSS might have a slight edge in its variety of advanced techniques. While Minitab focuses on ease of use, SPSS provides a more customizable interface.
* **Stata:** Similar to SPSS, Stata boasts a more extensive library of statistical methods and a powerful programming language for complex analysis. However, Stata's command-line interface can be daunting for beginners, while Minitab offers a gentler learning curve.

**Benefits of Using Minitab:**

* **User-Friendly Interface:** Easier to learn and use compared to Stata's command-line focus.
* **Six Sigma Integration:** Ideal for organizations implementing Six Sigma methodologies.
* **Focus on Quality Improvement:** Well-suited for process analysis and defect reduction.
* **Strong Visualizations:** Creates clear and effective data visualizations for presentations and reports.

**Who should use Minitab?**

Minitab is a good choice for users who:

* Need a user-friendly and accessible statistical software program.
* Are involved in Six Sigma quality improvement initiatives.
* Work in manufacturing, healthcare, or education where Minitab is widely used.
* Need strong data visualization capabilities.

* **Ease of use:** Minitab offers a gentler learning curve, while SPSS and Stata have steeper learning curves due to their advanced features. 
* **Statistical needs:** All three offer a wide range of statistical procedures, but SPSS and Stata might have a slight edge in very specialized areas. 
* **Six Sigma:** If you're involved in Six Sigma, Minitab is a strong choice with dedicated tools.
* **Data visualization:** All three create good visualizations, but consider the specific styles and customization options you need.

## MACHINE LEARNING
These machine-learning tools aim to make machine-learning techniques more accessible.

### IBM SPSS Modeler 
IBM's SPSS Modeler is one popular tool for building graphical machine learning models. Instead of requiring that users write code, it provides an intuitive interface where analysts can select the tasks that they would like the software to carry out and then connect them in a flowchart-style interface.

IBM SPSS Modeler, also known as just SPSS Modeler, is a software application from IBM specifically designed for data mining and machine learning tasks. It complements the broader data analysis capabilities of the general IBM SPSS suite by providing a visual and user-friendly platform for building predictive models. Here's a closer look at what SPSS Modeler offers:

**Focus on Data Mining and Machine Learning:**

* Unlike the general SPSS program that caters to a wide range of statistical analysis, SPSS Modeler is laser-focused on data mining and building predictive models. 
* It provides a streamlined workflow for data preparation, model building, deployment, and management.

**Visual Programming Interface:**

* One of the key strengths of SPSS Modeler is its intuitive visual interface. 
* You can drag-and-drop data processing steps, modeling algorithms, and visualizations to build your models without needing to write code. 
* This makes it accessible to a wider range of users, including data analysts and business professionals who might not have extensive programming experience.

**Wide Range of Algorithms:**

* SPSS Modeler offers a rich library of machine learning algorithms, including decision trees, neural networks, regression models, and more. 
* This allows you to tackle various data mining tasks, such as classification (predicting categories), regression (predicting continuous values), and association analysis (finding relationships between variables).

**Model Deployment and Management:**

*  SPSS Modeler goes beyond just building models. It provides tools to deploy your models into production environments where they can be used to score new data and generate insights. 
* Additionally, you can manage and monitor the performance of your models over time.

**Integration with Other Tools:**

* SPSS Modeler integrates seamlessly with other IBM products like IBM SPSS Statistics and IBM Cloud Pak for Data. 
* This allows you to leverage data analysis capabilities from other tools within your data science workflow.

**Benefits of Using SPSS Modeler:**

* **Faster Time to Insights:** The visual interface and pre-built components can significantly speed up the model building process compared to coding from scratch.
* **Increased Accessibility:** Makes data mining and machine learning accessible to a broader range of users. 
* **Improved Model Management:** Provides functionalities for deploying and managing models in production.
* **Scalability:** Handles large datasets and complex modeling tasks efficiently.

**Who Should Use SPSS Modeler?**

* **Data Analysts:** Build and refine predictive models for various data mining tasks.
* **Data Scientists:** Prototype and test machine learning models in a visual environment.
* **Business Analysts:** Gain insights from data through models without extensive coding knowledge.
* **Business Users:** Understand and interpret the results of data mining models.

SPSS Modeler is a valuable tool for organizations that want to leverage data mining and machine learning to extract insights from their data. Its user-friendly interface, combined with powerful algorithms and model management capabilities, makes it a popular choice for data scientists, analysts, and business users alike.

### RapidMiner
RapidMiner is another graphical machine learning tool that works in a manner similar to IBM SPSS Modeler. It offers access to hundreds of different algorithms that may be placed in a visually designed machine-learning workflow. RapidMiner is a comprehensive data science platform that offers a unique blend of features for data mining, machine learning, and predictive analytics. Here's a breakdown of what RapidMiner brings to the table:

**Focus on Ease of Use:**

Unlike some data science tools that require extensive coding expertise, RapidMiner emphasizes a visual approach.  Its core functionality revolves around building processes, which are essentially workflows created by dragging and dropping pre-built operators.  This intuitive interface makes it accessible to a wider range of users, including data analysts and business professionals with limited programming experience.

**Streamlined Data Science Workflow:**

RapidMiner encompasses the entire data science lifecycle, from data ingestion and preparation to model building, evaluation, and deployment. Here's a glimpse into its capabilities:

* **Data Integration:** Import data from various sources like databases, spreadsheets, and flat files.
* **Data Cleaning and Transformation:** Cleanse and prepare your data for analysis using a variety of operators for handling missing values, normalization, and data wrangling.
* **Machine Learning:**  Build and train a wide range of models, including decision trees, regression models, classification algorithms, and more. RapidMiner also supports integration with external libraries like scikit-learn for even more flexibility.
* **Model Evaluation:** Assess the performance of your models using various metrics and visualizations to identify the best fit for your data.
* **Deployment and Scoring:** Deploy your models into production environments to score new data and generate predictions.

**Open-source foundation with Paid Options:**

RapidMiner offers a free open-source edition with limitations on data size and processing power. However, paid subscriptions provide additional features, increased scalability, and access to advanced functionalities like text mining and integration with other platforms.

**Benefits of Using RapidMiner:**

* **Faster Development:** The visual interface and pre-built operators significantly speed up the data science workflow.
* **Increased Collaboration:**  The user-friendly nature fosters collaboration between data scientists, analysts, and business users. 
* **Scalability:** The paid versions cater to larger datasets and complex modeling tasks.
* **Open-source foundation:**  The free edition allows you to experiment and learn the platform before committing to a paid plan.

**Who Should Use RapidMiner?**

* **Data Analysts:** Quickly build and iterate on data mining and machine learning models.
* **Data Scientists:** Prototype and test complex models using the visual interface and integrate with external libraries.
* **Business Analysts:** Gain insights from data through models without needing to write code.
* **Business Users:** Understand and interpret the results of data mining models presented through clear visualizations.

RapidMiner stands out as a user-friendly and versatile platform that empowers users of various backgrounds to participate in the data science process. Its ability to handle the entire data science workflow, from data preparation to model deployment, makes it a valuable tool for organizations looking to extract knowledge from their data.
